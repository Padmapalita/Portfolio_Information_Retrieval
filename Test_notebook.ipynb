{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4280a656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, '02cvhI2v4wORCGTvWQxF7P', 0],\n",
       " [8, '03G8qxp2IYaaX1RqYFwPs9', 0],\n",
       " [7, '04536ZKxCGV88Yj0TT0oYM', 0],\n",
       " [7, '047y46T88lyQEHIEEVWxgy', 0],\n",
       " [6, '04P7on6BaQikQsMmTVlMNB', 1]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def get_qrels(filename):\n",
    "  # read the file from TrEC that contains the relevance scores\n",
    "  with open(filename) as f:\n",
    "      contents = f.read()\n",
    "  # shorten the episode ID and split the time segment into seperate field\n",
    "  lines = contents.replace('spotify:episode:','').replace('_','\\t').split(\"\\n\")\n",
    "  data = [line.split('\\t') for line in lines]\n",
    "  # create dataframe and remove the second column which seems to have no value\n",
    "  df = pd.DataFrame(data,columns = ['query_id', 'useless','episode','segment','relevance'])\n",
    "  df = df.drop('useless', axis=1)\n",
    "  df['relevance'] = df['relevance'].astype(int)\n",
    "  # the relevance scores are on scale 0-4, instead consider if it is relevant or not\n",
    "  df['binary'] = df['relevance'] > 0\n",
    "  df['binary'] = df['binary'].astype(int)\n",
    "  # if an episode has relevance at 'some' point then consider the whole episode to be relevant\n",
    "  df2 = df.groupby(['episode','query_id'])['binary'].max()\n",
    "  # adjusting the dataframe into a list with (query_id, document_id, judgement)\n",
    "  cols = ['query_id', 'episode', 'binary']\n",
    "  df2 = df2.reset_index()\n",
    "  df2['query_id'] = df2['query_id'].astype(int)\n",
    "  qrels = df2[cols].values.tolist()\n",
    "  return qrels\n",
    "\n",
    "qrels = get_qrels(\"Files\\/2020_train_qrels.list.txt\")\n",
    "qrels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bae4edf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_queries(filename):\n",
    "  # read the file from TrEC that contains the query titles\n",
    "  with open(filename) as f:\n",
    "    contents = f.read()\n",
    "  from bs4 import BeautifulSoup\n",
    "  soup = BeautifulSoup(contents)\n",
    "  query_list = [query.text for query in soup.find_all('description')]\n",
    "  # put the queries into a dictionary but need to start numbering at 1\n",
    "  queries = {i+1: val for i, val in enumerate(query_list)}\n",
    "  return queries\n",
    "queries = get_queries('Files\\podcasts_2020_topics_train.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa5141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'What were people saying about the spread of the novel coronavirus NCOV-19 in Wuhan at the end of 2019?',\n",
       " 2: 'What were people saying about Greta Thunberg’s sailing trip across the Atlantic Ocean in the fall of 2019 and its relationship to global climate change?',\n",
       " 3: 'In May 2019 astronomers released the first-ever picture of a black hole. I would like to hear some conversations and educational discussion about the science of astronomy, black holes, and of the picture itself.',\n",
       " 4: 'I remember hearing a podcast that had a story about a kid riding some kind of bird. I want to find it again.',\n",
       " 5: 'Someone told me about a podcast interview with Daniel Ek, CEO of Spotify, about the founding and early days of Spotify. I would like to find the show and episode that contains that interview. Other interviews with Ek are relevant as well.',\n",
       " 6: 'Former First Lady Michelle Obama’s memoir Becoming was published in early 2019. What were people saying about it?',\n",
       " 7: 'Anna Sorokina moved to New York City in 2013 and posed as wealthy German heiress Anna Delvey. In 2019 she was convicted of grand larceny, theft, and fraud. What were people saying about her, the charges, her trial, and New York socialite society in general?',\n",
       " 8: 'After Facebook’s Q4 2018 earnings call, what were experts’ predictions and expectations for its stock price in 2019? How did these predictions fare over time? Relevant material would include predictions immediately following the Jan 29, 2019, earnings call, and later actual stock performance that could be used to evaluate predictions.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b062c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "# read all the json files in the folder called Documents\n",
    "path = 'Documents/*'\n",
    "files = glob.glob(path)\n",
    "\n",
    "def get_transcripts():\n",
    "    transcripts = []\n",
    "    ep_IDs = []\n",
    "    titles = []\n",
    "    # loop through each of the files extracting data\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "          contents = json.load(f)\n",
    "        #   show_ID is slightly misleading name as would not be unique so renamed ep_ID\n",
    "          ep_ID = contents[\"showID\"]\n",
    "          ep_IDs.append(ep_ID)\n",
    "        # the transcript is a list so change to string\n",
    "        # will create a transcript+ later that also includes episode_description\n",
    "          transcript = ''.join(contents[\"transcript\"])\n",
    "          transcripts.append(transcript)\n",
    "          title = contents[\"show_name\"] + \" - \" + contents[\"episode_name\"]\n",
    "          titles.append(title)\n",
    "    return ep_IDs, transcripts, titles\n",
    "ep_IDs, corpus, titles = get_transcripts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e412ac64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0a0C2tbL45RMmL9EmEVC2R: Do I Have A Story For You! - Do I Have A Story For You!\n",
      "0a0C9jWzl6eUUhM6mxTwbn: Stay Classic: A Warcraft Podcast - Stay Classic: A Warcraft Podcast\n",
      "0A0f1WM7IttoJ61xzWCPK5: Vhite Rabbit Podcast - Vhite Rabbit Podcast\n",
      "0a0HuaT4Vm7FoYvccyRRQj: Back 2 HER  - Back 2 HER \n",
      "0a0Ikpt3GH8xSKaMZm4RYw: Pack-A-Day: Your Daily Packers Podcast - Pack-A-Day: Your Daily Packers Podcast\n",
      "0a0iyqjSgKKZ49eOKZYpY9: The All Things Mavs Podcast - The All Things Mavs Podcast\n",
      "0a0jLPxKIjaDYUaZPbhsWO: The Hawg Talk Podcast - The Hawg Talk Podcast\n",
      "0A0MxX8L2YZEZgJGApE7w4: Thai Endzone Podcast - Thai Endzone Podcast\n",
      "0A0rUcBRvpL436mIflNoVg: AFL Deep Dive - AFL Deep Dive\n",
      "0A0SL2WQhWgGV6phvwJgU7: Variant Podcast - Variant Podcast\n",
      "0A0SrnP0qm15L5Hv27sMAI: West Didsbury & Chorlton AFC - West Didsbury & Chorlton AFC\n",
      "0A0TSLljzX8akWoSYqF6Hm: Unsolved Murders: True Crime Stories - Unsolved Murders: True Crime Stories\n",
      "0A1bZfQ1C2FOUMfVJ7ugpa: Chompers - Chompers\n",
      "0a1edJv75fmg5bnYDgeZ0v: Light and Love - Light and Love\n",
      "0A1ejb0nplbJ7Z3qDF31D9: The Language Learning Show - The Language Learning Show\n",
      "0A1GTvNCft6B1bVD2Guioo: Nobody Asked You, Kevin! Podcast - Nobody Asked You, Kevin! Podcast\n",
      "0A1iNmwn0VjegiQ8kBXc4u: Cephalon Squared: A Warframe Podcast - Cephalon Squared: A Warframe Podcast\n",
      "0a1jXZ8LRPJRYWA9mHLW7w: Medscool Podcast - Medscool Podcast\n",
      "0A1lKioyXNKIBRGAzYMUQi: Recruiter Startup - Dualta Doherty Rec2rec. - Recruiter Startup - Dualta Doherty Rec2rec.\n",
      "0a1OhqAgMKmW5JwKWG7LbF: Embodied Astrology with Renee Sills - Embodied Astrology with Renee Sills\n",
      "0a1xawFR0oGJVP672q5ZuD: Life's amazing secrets - Life's amazing secrets\n",
      "0a1YNZdeCkdMTl61UQgs9M: The JRPG Report - The JRPG Report\n",
      "0A2B0uRupGRFkIszMIoni5: Embodied Astrology with Renee Sills - Embodied Astrology with Renee Sills\n",
      "0a2BQTox8cxHkTZQzkIpf4: The Wrestling-Wrestling Podcast - The Wrestling-Wrestling Podcast\n",
      "0A2jf2xBqee3KXv2AUfsp7: Breaking Beyond-Guided Meditation - Breaking Beyond-Guided Meditation\n",
      "0a2P0WKrPIzDSiB57ILdwN: Your Sleep Guru  - Your Sleep Guru \n",
      "0a2rz6SLuoQagFLypkidtg: The Modern Warrior Podcast  - The Modern Warrior Podcast \n",
      "0A2xwMoDNIlwkJGsaFq68I: Everything DFS Sports - Everything DFS Sports\n",
      "0A4HFH7rgoBjz44K0ZABXl: The Real Madrid Podcast  - The Real Madrid Podcast \n",
      "0a4HRdmYYB4jQrc0bGGPkS: AAA Pass: A Nation of Billions Podcast - AAA Pass: A Nation of Billions Podcast\n",
      "0A4rSk465szkl19Zb6UXA2: The High On Life Podcast - The High On Life Podcast\n",
      "0A4ztiz48hDGPJyf0HECoH: Free Neville Goddard  - Free Neville Goddard \n",
      "0A5BLuSdVS2uQCC8uiuO2N: Justbrandi - Justbrandi\n",
      "0a5cpbh8YLko5O9ZzhtaBa: X-Pac 12360 - A Wrestling Podcast - X-Pac 12360 - A Wrestling Podcast\n",
      "0a5DJZA0NWLIGWTMasTNYq: Tucker & Maura - Tucker & Maura\n"
     ]
    }
   ],
   "source": [
    "# index\n",
    "for i in range(len(ep_IDs)):\n",
    "  print(f'{ep_IDs[i]}: {titles[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80d77e36",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# vectorize and get vocabulary\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "# vectorize and get vocabulary\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "documents_vectorized = vectorizer.fit_transform(corpus)\n",
    "vocabulary = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b1bc8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '0a0HuaT4Vm7FoYvccyRRQj',\n",
       " 1: '0a5DJZA0NWLIGWTMasTNYq',\n",
       " 2: '0A0SL2WQhWgGV6phvwJgU7',\n",
       " 3: '0a4HRdmYYB4jQrc0bGGPkS',\n",
       " 4: '0A1lKioyXNKIBRGAzYMUQi',\n",
       " 5: '0A1iNmwn0VjegiQ8kBXc4u',\n",
       " 6: '0a0iyqjSgKKZ49eOKZYpY9',\n",
       " 7: '0A0SrnP0qm15L5Hv27sMAI',\n",
       " 8: '0a0jLPxKIjaDYUaZPbhsWO',\n",
       " 9: '0A0rUcBRvpL436mIflNoVg',\n",
       " 10: '0a2rz6SLuoQagFLypkidtg',\n",
       " 11: '0a1edJv75fmg5bnYDgeZ0v',\n",
       " 12: '0A1ejb0nplbJ7Z3qDF31D9',\n",
       " 13: '0A4rSk465szkl19Zb6UXA2',\n",
       " 14: '0A1bZfQ1C2FOUMfVJ7ugpa',\n",
       " 15: '0a1OhqAgMKmW5JwKWG7LbF',\n",
       " 16: '0A2B0uRupGRFkIszMIoni5',\n",
       " 17: '0A0MxX8L2YZEZgJGApE7w4',\n",
       " 18: '0a1xawFR0oGJVP672q5ZuD',\n",
       " 19: '0a2BQTox8cxHkTZQzkIpf4',\n",
       " 20: '0A2xwMoDNIlwkJGsaFq68I',\n",
       " 21: '0a1jXZ8LRPJRYWA9mHLW7w',\n",
       " 22: '0A2jf2xBqee3KXv2AUfsp7',\n",
       " 23: '0a1YNZdeCkdMTl61UQgs9M',\n",
       " 24: '0A0f1WM7IttoJ61xzWCPK5',\n",
       " 25: '0A5BLuSdVS2uQCC8uiuO2N',\n",
       " 26: '0A0TSLljzX8akWoSYqF6Hm',\n",
       " 27: '0a5cpbh8YLko5O9ZzhtaBa',\n",
       " 28: '0a0Ikpt3GH8xSKaMZm4RYw',\n",
       " 29: '0A4HFH7rgoBjz44K0ZABXl',\n",
       " 30: '0a0C2tbL45RMmL9EmEVC2R',\n",
       " 31: '0A1GTvNCft6B1bVD2Guioo',\n",
       " 32: '0a0C9jWzl6eUUhM6mxTwbn',\n",
       " 33: '0a2P0WKrPIzDSiB57ILdwN',\n",
       " 34: '0A4ztiz48hDGPJyf0HECoH'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = dict(enumerate(ep_IDs))\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cda719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>100th</th>\n",
       "      <th>102</th>\n",
       "      <th>1099</th>\n",
       "      <th>10d</th>\n",
       "      <th>10th</th>\n",
       "      <th>...</th>\n",
       "      <th>zip</th>\n",
       "      <th>ziploc</th>\n",
       "      <th>zipping</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>ëif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0a0HuaT4Vm7FoYvccyRRQj</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0a5DJZA0NWLIGWTMasTNYq</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0A0SL2WQhWgGV6phvwJgU7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0a4HRdmYYB4jQrc0bGGPkS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0A1lKioyXNKIBRGAzYMUQi</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10857 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        00  000  10  100  1000  100th  102  1099  10d  10th  \\\n",
       "0a0HuaT4Vm7FoYvccyRRQj   0    0   0    0     0      0    2     0    0     0   \n",
       "0a5DJZA0NWLIGWTMasTNYq   1    2   2    1     0      0    0     0    0     1   \n",
       "0A0SL2WQhWgGV6phvwJgU7   0    0   0    2     0      0    0     0    0     0   \n",
       "0a4HRdmYYB4jQrc0bGGPkS   0    0   0    2     0      0    0     0    0     0   \n",
       "0A1lKioyXNKIBRGAzYMUQi   0    1   0    1     0      0    0     0    0     0   \n",
       "\n",
       "                        ...  zip  ziploc  zipping  zodiac  zombie  zombies  \\\n",
       "0a0HuaT4Vm7FoYvccyRRQj  ...    0       0        0       5       0        0   \n",
       "0a5DJZA0NWLIGWTMasTNYq  ...    0       0        0       0       0        0   \n",
       "0A0SL2WQhWgGV6phvwJgU7  ...    0       0        0       0       1        0   \n",
       "0a4HRdmYYB4jQrc0bGGPkS  ...    0       0        0       0       0        0   \n",
       "0A1lKioyXNKIBRGAzYMUQi  ...    0       0        0       0       0        0   \n",
       "\n",
       "                        zone  zones  zoom  ëif  \n",
       "0a0HuaT4Vm7FoYvccyRRQj     0      0     0    0  \n",
       "0a5DJZA0NWLIGWTMasTNYq     0      0     0    0  \n",
       "0A0SL2WQhWgGV6phvwJgU7     0      0     0    0  \n",
       "0a4HRdmYYB4jQrc0bGGPkS     0      0     0    0  \n",
       "0A1lKioyXNKIBRGAzYMUQi     0      0     0    0  \n",
       "\n",
       "[5 rows x 10857 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(documents_vectorized.toarray(), columns=vocabulary)\n",
    "df.index = list(ep_IDs)\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e565de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25_IDF_df(df):\n",
    "  \"\"\"\n",
    "  This definition calculates BM25-IDF weights before hand as done last week\n",
    "  \"\"\"\n",
    "\n",
    "  dfs = (df > 0).sum(axis=0)\n",
    "  N = df.shape[0]\n",
    "  idfs = -np.log(dfs / N)\n",
    "  \n",
    "  k_1 = 1.2\n",
    "  b = 0.8\n",
    "  dls = df.sum(axis=1) \n",
    "  avgdl = np.mean(dls)\n",
    "\n",
    "  numerator = np.array((k_1 + 1) * df)\n",
    "  denominator = np.array(k_1 *((1 - b) + b * (dls / avgdl))).reshape(N,1) \\\n",
    "                         + np.array(df)\n",
    "\n",
    "  BM25_tf = numerator / denominator\n",
    "\n",
    "  idfs = np.array(idfs)\n",
    "\n",
    "  BM25_score = BM25_tf * idfs\n",
    "  return pd.DataFrame(BM25_score, columns=vocabulary)\n",
    "bm25_df = BM25_IDF_df(df)\n",
    "bm25_df.index = list(ep_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_ranking(query, bm25_df):\n",
    "  q_terms = query.split(' ')\n",
    "  q_terms_only = bm25_df[q_terms]\n",
    "  score_q_d = q_terms_only.sum(axis=1)\n",
    "  return sorted(zip(bm25_df.index.values, score_q_d.values),\n",
    "                key = lambda tup:tup[1],\n",
    "                reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688eb35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0A4rSk465szkl19Zb6UXA2', 0.4731324049359577),\n",
       " ('0a5DJZA0NWLIGWTMasTNYq', 0.47109886038189513),\n",
       " ('0a0C9jWzl6eUUhM6mxTwbn', 0.47094869404631795),\n",
       " ('0A0SL2WQhWgGV6phvwJgU7', 0.46786118578720803),\n",
       " ('0a1edJv75fmg5bnYDgeZ0v', 0.46589413105935606),\n",
       " ('0a0HuaT4Vm7FoYvccyRRQj', 0.4633682283846159),\n",
       " ('0a5cpbh8YLko5O9ZzhtaBa', 0.46252535758159874),\n",
       " ('0A0SrnP0qm15L5Hv27sMAI', 0.45977832615467756),\n",
       " ('0a0C2tbL45RMmL9EmEVC2R', 0.4589673582570983),\n",
       " ('0a0Ikpt3GH8xSKaMZm4RYw', 0.4584589742556114),\n",
       " ('0a4HRdmYYB4jQrc0bGGPkS', 0.45655152906766344),\n",
       " ('0A4ztiz48hDGPJyf0HECoH', 0.456030891044336),\n",
       " ('0A1lKioyXNKIBRGAzYMUQi', 0.45402781152438515),\n",
       " ('0a2BQTox8cxHkTZQzkIpf4', 0.4512574559199662),\n",
       " ('0a0jLPxKIjaDYUaZPbhsWO', 0.4507364639662578),\n",
       " ('0A2B0uRupGRFkIszMIoni5', 0.4500095045285661),\n",
       " ('0A1ejb0nplbJ7Z3qDF31D9', 0.4479978389630656),\n",
       " ('0a1jXZ8LRPJRYWA9mHLW7w', 0.44690753936068783),\n",
       " ('0A0rUcBRvpL436mIflNoVg', 0.4450037898419007),\n",
       " ('0a2rz6SLuoQagFLypkidtg', 0.43891998882658473),\n",
       " ('0A1iNmwn0VjegiQ8kBXc4u', 0.4376136951485642),\n",
       " ('0a1OhqAgMKmW5JwKWG7LbF', 0.4332030883911565),\n",
       " ('0A1GTvNCft6B1bVD2Guioo', 0.428208540566946),\n",
       " ('0A0TSLljzX8akWoSYqF6Hm', 0.4060224151763546),\n",
       " ('0a1YNZdeCkdMTl61UQgs9M', 0.3658095995185894),\n",
       " ('0a0iyqjSgKKZ49eOKZYpY9', 0.34235889215262244),\n",
       " ('0A5BLuSdVS2uQCC8uiuO2N', 0.3340276037305989),\n",
       " ('0A2xwMoDNIlwkJGsaFq68I', 0.24320800206521903),\n",
       " ('0A1bZfQ1C2FOUMfVJ7ugpa', 0.0),\n",
       " ('0A0MxX8L2YZEZgJGApE7w4', 0.0),\n",
       " ('0a1xawFR0oGJVP672q5ZuD', 0.0),\n",
       " ('0A2jf2xBqee3KXv2AUfsp7', 0.0),\n",
       " ('0A0f1WM7IttoJ61xzWCPK5', 0.0),\n",
       " ('0A4HFH7rgoBjz44K0ZABXl', 0.0),\n",
       " ('0a2P0WKrPIzDSiB57ILdwN', 0.0)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this produces error\n",
    "# for count, query in enumerate(queries.values()):\n",
    "#   print(f'Query {count}: {query}')\n",
    "#   print('')\n",
    "#   print(retrieve_ranking(query, bm25_df))\n",
    "#   print('')\n",
    "\n",
    "# retrieve for a known word\n",
    "retrieve_ranking('people', bm25_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34542650",
   "metadata": {},
   "source": [
    "# Unit testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e291fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import unittest\n",
    "\n",
    "class test_Tom(unittest.TestCase):\n",
    "\n",
    "    def test_get_qrels(self):\n",
    "        qrels_file = 'qrels_test_file.txt'\n",
    "        qrels_result = [[7, '1xxxx', 1],\n",
    "                        [8, '2xxxx', 2],\n",
    "                        [9, '3xxxx', 0]]\n",
    "        self.assertCountEqual(get_qrels(qrels_file),\n",
    "                              qrels_result)\n",
    "\n",
    "    def test_get_queries(self):\n",
    "        queries_file = 'queries_test_file.xml'\n",
    "        queries_result = {1 : 'How do I get fit?',\n",
    "                          2 : 'What is Barack Obamas middle name?'}\n",
    "        self.assertDictEqual(get_queries(queries_file),\n",
    "                             queries_result)\n",
    "\n",
    "    def test_get_transcripts(self):\n",
    "        path = '/Transcripts/*'\n",
    "        files = glob.glob(path) \n",
    "        ep_IDs_result = [\"1a\", \"2b\"]\n",
    "        corpus_result = [\n",
    "            \"Hi and welcome to this podcast about podcasts.Today, we will be talking about podcasts.\",\n",
    "            \"It was probably misleading to call this a football podcast. Episode 1 will be about cheese, and I'm not promising it will ever actually come round to football.\"\n",
    "        ]\n",
    "        titles_result = [\n",
    "            \"The podcast show - The first episode\"\n",
    "            \"Football or something - Let's not bother starting with football.\"\n",
    "            ]\n",
    "        ep_IDs, corpus, titles = get_transcripts()\n",
    "        self.assertCountEqual(ep_IDs, ep_IDs_result)\n",
    "        self.assertCountEqual(corpus, corpus_result)\n",
    "        self.assertCountEqual(titles, titles_result)\n",
    "\n",
    "\n",
    "    def test_BM25_IDF_df(self):\n",
    "        import pandas.testing as pd_testing\n",
    "\n",
    "        doc_index_dict = {\n",
    "            'obama' : [0, 0, 1, 0, 1],\n",
    "            'middle' : [1, 0, 0, 0, 0],\n",
    "            'spotify' : [0, 0, 0, 1, 0]\n",
    "        }\n",
    "        doc_index_df = pd.DataFrame(doc_index_dict)\n",
    "        doc_index_episode_ids = ['0xxxx','1xxxx', '2xxxx', '3xxxx', '4xxxx']\n",
    "        doc_index_df.index = doc_index_episode_ids\n",
    "\n",
    "        doc_index_result_dict = {\n",
    "            'obama' : [0, 0, 0.9881566716289908, 0, 0.9881566716289908],\n",
    "            'middle' : [1.7356683369387358, 0, 0, 0, 0],\n",
    "            'spotify' : [0, 1.0499164636058027, 0, 1, 0.9881566716289908]                    \n",
    "        }\n",
    "        doc_index_result_df = pd.DataFrame(doc_index_result_dict)\n",
    "\n",
    "        pd_testing.assert_frame_equal(Tom.BM25_IDF_df(doc_index_df),\n",
    "                                      doc_index_result_df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c347c8f9a7ef94e4c9e03b4513be7835ed18f45b99a2a817fb579f408b867b16"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
